[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "README",
    "section": "",
    "text": "This website is buggy\n\n     \n\n\n\nHow Predictive Models Help British Airways Acquire Customers\n\n\nAuthor: Simontagbor\n\n\nLast updated: 2024-01-20\n\n\n\n\nThis project is about combating the problem of unstable buying cycle of British Airways customers using the power of predictive modeling. The project was done as part of a job simulation as a Junior Data Scientist at British Airways.\n\nIn this Interactive Notebook I will show how I trained a predictive model with Random Forest on 5000 entries for customer booking data from British Airways.\n\n\nThe goal is to help British Airways acquire customers before the holidays.\n\n\nThe project is divided into 4 parts:\n\n\n\nExploratory Data Analysis\n\n\nData Cleaning and Preparation\n\n\nFeature Engineering\n\n\nModeling and Evaluation\n\n\nThe source code for this project can be found at my github page.\n\nYou can also find the Jupyter notebook and the dataset on my kaggle page.\n\n\n\n\nProject Overview\n\nFor this project I completed the following tasks:\n\nPerformed Exploratory Data Analysis.\nCleaned and Prepared the data for modeling.\nPerformed Feature Engineering.\nBuilt and Evaluated models.\nTest the model on unseen data.\nInterpret the model results and make predictions.\n\n\nProject Dependencies\n\n\n\nShow the code\n# import project libraries\nimport pandas as pd\nimport numpy as np # for linear algebra\nimport math # for math operations \n\nimport seaborn as sns # for plotting\n\n# handling files\nimport os \nimport sys \n\n# data preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\n# visualisation libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt # for plotting\nimport squarify # for tree maps\n\n\n\nExploratory Data Analysis\n\n\nTo understand the data, I performed exploratory data analysis. I used the following techniques to understand the data:\n\n\nVisual inspection of data.\nExploratory Data Visualizations.(Univariate, Bivariate and Multivariate analysis)\n\n\nVisual Inspection of Data\n\n\ndf = pd.read_csv(\"predictive_model_data/customer_booking.csv\", encoding=\"ISO-8859-1\")\ndf.head()\n\n\n\n\n\n\n\n\nnum_passengers\nsales_channel\ntrip_type\npurchase_lead\nlength_of_stay\nflight_hour\nflight_day\nroute\nbooking_origin\nwants_extra_baggage\nwants_preferred_seat\nwants_in_flight_meals\nflight_duration\nbooking_complete\n\n\n\n\n0\n2\nInternet\nRoundTrip\n262\n19\n7\nSat\nAKLDEL\nNew Zealand\n1\n0\n0\n5.52\n0\n\n\n1\n1\nInternet\nRoundTrip\n112\n20\n3\nSat\nAKLDEL\nNew Zealand\n0\n0\n0\n5.52\n0\n\n\n2\n2\nInternet\nRoundTrip\n243\n22\n17\nWed\nAKLDEL\nIndia\n1\n1\n0\n5.52\n0\n\n\n3\n1\nInternet\nRoundTrip\n96\n31\n4\nSat\nAKLDEL\nNew Zealand\n0\n0\n1\n5.52\n0\n\n\n4\n2\nInternet\nRoundTrip\n68\n22\n15\nWed\nAKLDEL\nIndia\n1\n0\n1\n5.52\n0\n\n\n\n\n\n\n\nThe dataframe object has a .head() method that allows us to view a specified number of rows in the dataset. This method came in handy for visual inspection of the dataset and to get a feel of the data.\n\n# retrieve a summarised info about the columns\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 14 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   num_passengers         50000 non-null  int64  \n 1   sales_channel          50000 non-null  object \n 2   trip_type              50000 non-null  object \n 3   purchase_lead          50000 non-null  int64  \n 4   length_of_stay         50000 non-null  int64  \n 5   flight_hour            50000 non-null  int64  \n 6   flight_day             50000 non-null  object \n 7   route                  50000 non-null  object \n 8   booking_origin         50000 non-null  object \n 9   wants_extra_baggage    50000 non-null  int64  \n 10  wants_preferred_seat   50000 non-null  int64  \n 11  wants_in_flight_meals  50000 non-null  int64  \n 12  flight_duration        50000 non-null  float64\n 13  booking_complete       50000 non-null  int64  \ndtypes: float64(1), int64(8), object(5)\nmemory usage: 5.3+ MB\n\n\nThe summary on the columns indicates that there are:\n\n\n14 unique columns\n\n\n5000 rows\n\n\nZero null values (which is good!)\n\n\nThree unique data types (int64, float64 and object)\n\n\nHere is a more detailed data description, explaining exactly what each column means:\n\n\nClick to expand!\n\n\n\nnum_passengers = the number of passengers associated with a particular flight booking.\n\n\nsales_channel = indicates the channel through which the flight booking was made.\n\n\ntrip_type = indicates the trip Type (Round Trip, One Way, Circle Trip)\n\n\npurchase_lead = the number of days between travel date and booking date\n\n\nlength_of_stay = the number of days spent at destination\n\n\nflight_hour = represents the scheduled departure or arrival time of the flight\n\n\nflight_day = day of week of flight departure\n\n\nroute = origin -&gt; represents the destination flight route\n\n\nbooking_origin = shows the country from where booking was made\n\n\nwants_extra_baggage = indicates if the customer wanted extra baggage in the booking\n\n\nwants_preferred_seat = indicates if the customer wanted a preferred seat in the booking\n\n\nwants_in_flight_meals = if the customer wanted in-flight meals in the booking\n\n\nflight_duration = shows the total duration of flight (in hours)\n\n\nbooking_complete = indicates if the customer completed the booking\n\n\n\nConsidering the unique data types, I decided to perform label encoding to convert the categorical variables to numerical variables. converting categorical variables to numerical representations will allow me to use machine learning algorithms effectively.\n\nIn our case the ideal candidate for label encoding is: flight_day. this is because it is currently a nominal variable with no order or hierarchy.\n\n\ndf[\"flight_day\"].unique()\n\narray(['Sat', 'Wed', 'Thu', 'Mon', 'Sun', 'Tue', 'Fri'], dtype=object)\n\n\n\nAs can be seen the current days of the week have no order or hierarchy.\n\n\nTo encode this variable I will map the days of the week to the following numerical representations:\n\n\nmapping = {\n    \"Mon\": 1,\n    \"Tue\": 2,\n    \"Wed\": 3,\n    \"Thu\": 4,\n    \"Fri\": 5,\n    \"Sat\": 6,\n    \"Sun\": 7,\n}\n\n# set the new values\ndf[\"flight_day\"] = df[\"flight_day\"].map(mapping)\n\n\ndf[\"flight_day\"].unique()\n\narray([6, 3, 4, 1, 7, 2, 5])\n\n\nAs can be seen the fligt_days column has been successfully encoded.\n\nSummary Statistics\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nnum_passengers\npurchase_lead\nlength_of_stay\nflight_hour\nflight_day\nwants_extra_baggage\nwants_preferred_seat\nwants_in_flight_meals\nflight_duration\nbooking_complete\n\n\n\n\ncount\n50000.000000\n50000.000000\n50000.00000\n50000.00000\n50000.000000\n50000.000000\n50000.000000\n50000.000000\n50000.000000\n50000.000000\n\n\nmean\n1.591240\n84.940480\n23.04456\n9.06634\n3.814420\n0.668780\n0.296960\n0.427140\n7.277561\n0.149560\n\n\nstd\n1.020165\n90.451378\n33.88767\n5.41266\n1.992792\n0.470657\n0.456923\n0.494668\n1.496863\n0.356643\n\n\nmin\n1.000000\n0.000000\n0.00000\n0.00000\n1.000000\n0.000000\n0.000000\n0.000000\n4.670000\n0.000000\n\n\n25%\n1.000000\n21.000000\n5.00000\n5.00000\n2.000000\n0.000000\n0.000000\n0.000000\n5.620000\n0.000000\n\n\n50%\n1.000000\n51.000000\n17.00000\n9.00000\n4.000000\n1.000000\n0.000000\n0.000000\n7.570000\n0.000000\n\n\n75%\n2.000000\n115.000000\n28.00000\n13.00000\n5.000000\n1.000000\n1.000000\n1.000000\n8.830000\n0.000000\n\n\nmax\n9.000000\n867.000000\n778.00000\n23.00000\n7.000000\n1.000000\n1.000000\n1.000000\n9.500000\n1.000000\n\n\n\n\n\n\n\n\nSome Highlights from The summary statistics\n\n\nThe majority of bookings involve a small number of passengers, with 75% of the bookings having either 1 or 2 passengers. However, there are cases with up to 9 passengers.\nThe average purchase lead time is approximately 85 days, indicating that customers typically book flights well in advance of their travel dates. Inferring from the significant difference between the mean and the median(50th percentile) the distribution is positively-skewed. The predictive model will be trained with Random Forest Algorithm so it is important to note that this algorithm is not sensitive to outliers.\nIt’s also interesting to point out that the average length of stay is approximately 23 days, indicating that customers typically stay at their destination for a short period of time. However, the maximum length of stay is 778 days, suggesting some outliers or potentially long-term bookings.\n\n\nFurther inspection of the data will help us understand the data better.\n\n\nChecking for Missing Values\n\n\n### Check for Nmissing values\ndf.isnull().sum()\n\nnum_passengers           0\nsales_channel            0\ntrip_type                0\npurchase_lead            0\nlength_of_stay           0\nflight_hour              0\nflight_day               0\nroute                    0\nbooking_origin           0\nwants_extra_baggage      0\nwants_preferred_seat     0\nwants_in_flight_meals    0\nflight_duration          0\nbooking_complete         0\ndtype: int64\n\n\n\nExploratory Visualizations of the data\n\nI perfomed further exploratory data analysis using visualizations. I used the following techniques to understand the data:\n\nUnivariate Analysis\nBivariate Analysis\nMultivariate Analysis\n\n\nUnivariate analysis\n\nMy goal for this initial step was to understand the distribution of each variable in the dataset. I used the following techniques to understand the distribution:\n\ndistribution plots\n\nhopefully, patterns and outliers would emerge from this analysis that would help me understand the data better.\n\nnum_passengers\npurchase_lead\nflight_day\nSales_channel\ntrip_type\nbooking_origin\nflight_duration\nbooking_complete\nroute\n\nI examined the above selected variables in the dataset using distribution plots. The plots below show the distribution of the following variables:\n\n# source code for visualisation can be found \n# at my Github gist:https://gist.github.com/Simontagbor/643a81b10ed2bd0d8d6542db8ddc53e2 \n\n\n\nShow the code\n# Convert numeric weekday to corresponding day names\nweekday_mapping = {1: 'Mon', 2: 'Tue', 3: 'Wed',\n                    4: 'Thu', 5: 'Fri', 6: 'Sat', 7: 'Sun'}\n# Set up the grid layout\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\nfig.suptitle('Distribution Plots for Selected Variables', \n             fontsize=16)\n# Create a copy of the DataFrame\ndf_copy = df.copy()\n# Apply the mapping to the copy\ndf_copy['flight_day'] = df_copy['flight_day'].map(weekday_mapping)\n# Plotting num_passengers\nsns.histplot(data=df_copy, x='num_passengers',\n             kde=True, color='skyblue',\n            ax=axes[0, 0])\naxes[0, 0].set_title('Distribution of num_passengers')\naxes[0, 0].set_xlabel('Number of Passengers')\naxes[0, 0].set_ylabel('Frequency')\n# Plotting purchase_lead\nsns.histplot(data=df_copy, x='purchase_lead', \n             kde=True, color='salmon', \n             ax=axes[0, 1])\naxes[0, 1].set_title('Distribution of purchase_lead')\naxes[0, 1].set_xlabel('Purchase Lead Time (days)')\naxes[0, 1].set_ylabel('Frequency')\n# Plotting flight_day with day labels\nsns.histplot(data=df_copy, x='flight_day',\n            kde=True, color='lightgreen', \n            ax=axes[0, 2])\naxes[0, 2].set_title('Distribution of flight_day')\naxes[0, 2].set_xlabel('Flight Day')\naxes[0, 2].set_ylabel('Frequency')\n# Plotting Sales_channel\nsns.countplot(data=df_copy, x='sales_channel',\n               color='orange', ax=axes[1, 0])\naxes[1, 0].set_title('Distribution of Sales_channel')\naxes[1, 0].set_xlabel('Sales Channel')\naxes[1, 0].set_ylabel('Count')\n# Plotting trip_type\nsns.countplot(data=df_copy, x='trip_type',\n               color='purple', ax=axes[1, 1])\naxes[1, 1].set_title('Distribution of trip_type')\naxes[1, 1].set_xlabel('Trip Type')\naxes[1, 1].set_ylabel('Count')\n# Adding the treemap for booking_origin (top 10 countries only)\ntop_10_origin_counts = df_copy['booking_origin'].value_counts().nlargest(10)\naxes[1, 2].axis('off')  # Turn off axis for treemap\nsquarify.plot(sizes=top_10_origin_counts, \n              label=top_10_origin_counts.index, \n              color=sns.color_palette(\"Set3\"), ax=axes[1, 2])\n# Adding title to the treemap\naxes[1, 2].set_title('Top 10 Booking Origins - Treemap')\n# Plotting flight_duration\nsns.histplot(data=df_copy, x='flight_duration', \n             kde=True, color='skyblue', \n             ax=axes[2, 0])\naxes[2, 0].set_title('Distribution of flight_duration')\naxes[2, 0].set_xlabel('Flight Duration')\naxes[2, 0].set_ylabel('Frequency')\n# Plotting booking_complete\nsns.histplot(data=df_copy, x='booking_complete',\n             kde=True, color='olive',\n             ax=axes[2, 1])\naxes[2, 1].set_title('Distribution of booking_complete')\naxes[2, 1].set_xlabel('Booking Complete')\naxes[2, 1].set_ylabel('Frequency')\n# Plotting route\n# Get the top 10 routes\ntop_10_routes = df_copy['route'].value_counts().nlargest(10)\n# Plotting route\naxes[2, 2].axis('off')  # Turn off axis for treemap\nsquarify.plot(sizes=top_10_routes, \n              label=top_10_routes.index, \n              color=sns.color_palette(\"Set3\"),\n              ax=axes[2, 2])\naxes[2, 2].set_title('Top 10 Routes - Treemap')\n# Adjust layout\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n# Show the plots\nplt.show()\n# Plotting flight_day with day labels\nsns.histplot(data=df_copy, x='flight_day',\n            kde=True, color='lightgreen', \n            ax=axes[0, 2])\naxes[0, 2].set_title('Distribution of flight_day')\naxes[0, 2].set_xlabel('Flight Day')\naxes[0, 2].set_ylabel('Frequency')\n\n# Plotting Sales_channel\nsns.countplot(data=df_copy, x='sales_channel',\n               color='orange', ax=axes[1, 0])\naxes[1, 0].set_title('Distribution of Sales_channel')\naxes[1, 0].set_xlabel('Sales Channel')\naxes[1, 0].set_ylabel('Count')\n# Plotting trip_type\nsns.countplot(data=df_copy, x='trip_type',\n               color='purple', ax=axes[1, 1])\naxes[1, 1].set_title('Distribution of trip_type')\naxes[1, 1].set_xlabel('Trip Type')\naxes[1, 1].set_ylabel('Count')\n\n# Adding the treemap for booking_origin (top 10 countries only)\ntop_10_origin_counts = df_copy['booking_origin'].value_counts().nlargest(10)\naxes[1, 2].axis('off')  # Turn off axis for treemap\nsquarify.plot(sizes=top_10_origin_counts, \n              label=top_10_origin_counts.index, \n              color=sns.color_palette(\"Set3\"), ax=axes[1, 2])\n# Adding title to the treemap\naxes[1, 2].set_title('Top 10 Booking Origins - Treemap')\n# Plotting flight_duration\nsns.histplot(data=df_copy, x='flight_duration', \n             kde=True, color='skyblue', \n             ax=axes[2, 0])\naxes[2, 0].set_title('Distribution of flight_duration')\naxes[2, 0].set_xlabel('Flight Duration')\naxes[2, 0].set_ylabel('Frequency')\n\n# Plotting booking_complete\nsns.histplot(data=df_copy, x='booking_complete',\n             kde=True, color='olive',\n             ax=axes[2, 1])\naxes[2, 1].set_title('Distribution of booking_complete')\naxes[2, 1].set_xlabel('Booking Complete')\naxes[2, 1].set_ylabel('Frequency')\n# Plotting route\n# Get the top 10 routes\ntop_10_routes = df_copy['route'].value_counts().nlargest(10)\n# Plotting route\naxes[2, 2].axis('off')  # Turn off axis for treemap\nsquarify.plot(sizes=top_10_routes, \n              label=top_10_routes.index, \n              color=sns.color_palette(\"Set3\"),\n              ax=axes[2, 2])\naxes[2, 2].set_title('Top 10 Routes - Treemap')\n# Adjust layout\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n# Show the plots\nplt.show()\n\n#| code-fold: true\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\nClick to Read My Observations!\n\nThe Univariate analysis revealed the following insights: the distribution of num_passengers is positively skewed with a long tail. This indicates that the majority of bookings involve a small number of passengers.\nThe distribution of purchase_lead is positively skewed with a long tail. This means that most passengers book their flights well in advance of their travel dates.for the purposes of predicting buying behaviour it informs my hypothesis that the longer the purchase lead time the more likely the customer to not complete the booking.\nThe distribution of flight_day is uniform. This means that the bookings are evenly distributed across the days of the week. This is an interesting insight because it may suggest that the day of the week does not have a significant impact on the buying behaviour of customers.\nThe distribution of sales_channel is skewed to the right. This means that the majority of bookings are made through the online channel. The right skewness indicates that there might be a long tail of other channels, but they are less frequent.\nThe distribution of trip_type is also positively-skewed, indicating that the majority of bookings are for round trips. While this suggests a prevalent preference for round trips, it is essential to conduct further bivariate analysis to confirm whether trip type significantly influences customer buying behavior.”\nThe majority of flight_origins are from Australia, while there are relatively even distributions among the remaining origins. This observation may suggest a low correlation between flight origin and buying behavior. However, further bivariate analysis will be conducted to confirm this hypothesis.\nSimilarly the majority of route is AKLKUL indicating that the majority of customers book flights from Auckland to Kuala Lumpur. This is a very interesting insight because it suggests that there is a significant number of customers who book flights from Auckland to Kuala Lumpur.\nThe distribution of flight_duration is quite skewed to the left. This means that a significant number of flights are short-haul flights. This observation may suggest a low correlation between flight duration and buying behavior. However, further bivariate analysis will be conducted to confirm this hypothesis.\nMajority of booking_complete is 0 indicating that the majority of customers do not complete the booking. This is a very interesting insight because it suggests that there is a significant number of customers who do not complete the booking.\n\n\nBivariate analysis Using Heat Maps\n\nMy goal for this step was to understand the relationship between each variable and the target variable. For the purposes of this project, the target variable is booking_complete. Considering that there are 14 variables I decided to use heatmaps to visualize the correlation between each variable and the target variable.\nI chose the heatmap because it allowed me to visualize the correlation between each variable and the target variable in one plot. This is a very efficient way to visualize the correlation between variables.\nBefore the Bivariate analysis, I Had some hypotheses about the relationship between the variables and the target variable. The hypotheses were as follows:\n\nThe longer the purchase_lead time the more likely the customer to not complete the booking.\nThe flight_day does not have a significant impact on the buying behaviour of customers.\nThe trip_type does not have a significant impact on the buying behaviour of customers.\nThe flight_origin does not have a significant impact on the buying behaviour of customers.\nthe sales_channel may have a significant impact on the buying behaviour of customers.\n\n\nPreprocessing the data for Bivariate analysis\n\nSome of the variables in the dataset are categorical variables that have no inherent order.The correalation matrix which will be used for analysis requires all variables to have numerical representations. Hence the need to label encode the remaining nominal variables:\n\nTo perform bivariate analysis on these variables I need to convert them to numerical representations using one-hot encoding. I used the pandas get_dummies() method to perform one-hot encoding on the following variables:\n\n\n\nsales_channel\n\n\ntrip_type\n\n\nbooking_origin\n\n\nroute\n\n\n\n\nShow the code\n# label encoding\nlabel_encoder = LabelEncoder()\n\n# Apply label encoding to categorical variables\ndf_copy['booking_origin'] = label_encoder.fit_transform(df_copy['booking_origin'])\ndf_copy['route'] = label_encoder.fit_transform(df_copy['route'])\ndf_copy['sales_channel'] = label_encoder.fit_transform(df_copy['sales_channel'])\ndf_copy['trip_type'] = label_encoder.fit_transform(df_copy['trip_type'])\ndf_copy['flight_day'] = label_encoder.fit_transform(df_copy['flight_day'])\n\ndf_copy.head()\n# check the data types\ndf.dtypes\n\n\nnum_passengers             int64\nsales_channel             object\ntrip_type                 object\npurchase_lead              int64\nlength_of_stay             int64\nflight_hour                int64\nflight_day                 int64\nroute                     object\nbooking_origin            object\nwants_extra_baggage        int64\nwants_preferred_seat       int64\nwants_in_flight_meals      int64\nflight_duration          float64\nbooking_complete           int64\ndtype: object\n\n\nNow that the categorical variables have been successfully encoded. I can proceed to perform bivariate analysis.\n\n# perform Bivariate analysis using heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(df_copy.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix of 14 Variables on customer booking behaviour',\n           fontsize=16)\n\n# Show the plot\nplt.show()\n\n\n\n\n\nMy Observations of Relationship between Booking Completions and 13 Other Variables\n\n\n\nClick to Read Detailed Observations!\n\n\n\nBooking Origin:\n\n\nCorrelation Coefficient: 0.13\n\n\nInterpretation: There’s a notable positive relationship (0.13) between booking origin and booking completions, suggesting that specific booking origins play a relatively significant role in determining completion likelihood.\n\n\n\n\nFlight Duration:\n\n\nCorrelation Coefficient: -0.11\n\n\nInterpretation: The negative correlation coefficient (-0.11) implies that longer flight durations may be associated with a lower likelihood of booking completions.\n\n\n\n\nWants Extra Baggage:\n\n\nCorrelation Coefficient: 0.068\n\n\nInterpretation: With a positive coefficient (0.068), the desire for extra baggage seems to have a fair contribution to the likelihood of completing bookings.\n\n\n\n\nLength of Stay:\n\n\nCorrelation Coefficient: -0.042\n\n\nInterpretation: A slightly negative correlation coefficient (-0.042) indicates a modest reverse relationship between length of stay and booking completion.\n\n\n\n\nWants Inflight Meals and Trip Type:\n\n\nCorrelation Coefficient: Positive (Same for both)\n\n\nInterpretation: Both exhibit a positive contribution to booking completion, suggesting that customers expressing interest in inflight meals or specific trip types may be slightly more likely to complete their bookings.\n\n\n\n\n\n\nIn summary, analysis of the heat map reveals key correlations with booking completions. Notably, specific booking origins play a significant role (correlation coefficient of 0.13), while longer flight durations show a negative correlation (-0.11) with completion likelihood. Surprisingly, the desire for extra baggage positively contributes (correlation coefficient of 0.068), while length of stay exhibits a modest reverse relationship (-0.042). Customer preferences for inflight meals and certain trip types positively influence completion likelihood. These findings provide valuable insights for further exploration and feature engineering in predictive modeling efforts.\n\n\n\nModeling With Random Forest\n\n\nTo build a predictive model to predict the likelihood of booking completions, I used the Random Forest algorithm. I chose Random Forest Algorithm because of it’s ensemble approach to learning to predict the relationships between the variables and the target variable. It’s like having multiple experts(decision trees) in the room to help you make a decision.\n\n\nThe Random Forest algorithm is also very robust to outliers and noise in the data. This is a very important feature because the dataset has some outliers and noise which refers to a random error or variance in a measured variable.\n\n\nPreprocessing the data for predictive modeling\n\n\nBefore proceeding to training the model:\n\n\n\nOne-hot encode the categorical variables:\n\n\n\nsales_channel\n\n\ntrip_type\n\n\nbooking_origin\n\n\nroute\n\n\n\nSplit the data into training and testing datasets.\n\n\n\nOne Hot Encoding Booking Data\n\nSome of the variables in the dataset are categorical variables that have no inherent order. Hence the need to label encode the remaining nominal variables:\n\nI needed to convert them to numerical representations using one-hot encoding. I used the pandas get_dummies() method to perform one-hot encoding on the selected variables:\n\n\n\nShow Label Encoding code\n# label encode the categorical variables\n\n# One-Hot Encode Nominal Variables\ndf = pd.get_dummies(df, columns=['sales_channel', \n                                 'trip_type', \n                                 'booking_origin', \n                                 'route'], \n                    drop_first=True)\n\n\nAccording to the pandas documentation, the get_dummies() method converts categorical variables into dummy/indicator variables. This means that each categorical variable will be converted into a numerical representation. The numerical representation will be in the form of a binary vector with a 1 representing the presence of a category and a 0 representing the absence of a category.\nlet’s comfirm that the categorical variables have been successfully encoded.\n\ndf.head()\n\n\n\n\n\n\n\n\nnum_passengers\npurchase_lead\nlength_of_stay\nflight_hour\nflight_day\nwants_extra_baggage\nwants_preferred_seat\nwants_in_flight_meals\nflight_duration\nbooking_complete\n...\nroute_TGGXIY\nroute_TPETRZ\nroute_TPETWU\nroute_TPEURT\nroute_TPEVTE\nroute_TRZWUH\nroute_TRZXIY\nroute_TWUWUH\nroute_TWUXIY\nroute_URTXIY\n\n\n\n\n0\n2\n262\n19\n7\n6\n1\n0\n0\n5.52\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n1\n112\n20\n3\n6\n0\n0\n0\n5.52\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n2\n243\n22\n17\n3\n1\n1\n0\n5.52\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n1\n96\n31\n4\n6\n0\n0\n1\n5.52\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n2\n68\n22\n15\n3\n1\n0\n1\n5.52\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 914 columns\n\n\n\n\nSplit Data for Training and Testing\n\nthe next step is to split the data into training and testing datasets. I used the train_test_split() method from the sklearn.model_selection module to split the data into training and testing datasets. The train_test_split() method takes the following arguments:\n\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop('booking_complete', axis=1)  # Features (excluding the target variable)\ny = df['booking_complete']  # Target variable\n\n# Stratify the split to ensure that the proportion of the target variable is the same in both sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nX: the features to be used for training the model. In this case, the features are all the columns in the dataset except the target variable booking_complete.\ny: the target variable to be predicted. In this case, the target variable is booking_complete.\nX_train: the features to be used for training the model.\nX_test: the features to be used for testing the model.\ny_train: the target variable to be used for training the model.\ny_test: the target variable to be used for testing the model.\nstratify: this argument is set to y to ensure that the proportion of the target variable is the same in both the training and testing datasets. I noticed a significant imbalance in the target variable, so it was important to ensure that the imbalance was reflected in both the training and testing datasets.\n\nTraining the Model\n\n\nNow that the data has been successfully preprocessed, I can proceed to train the model. I used the RandomForestClassifier() class from the sklearn.ensemble module to train the model. The RandomForestClassifier() method takes the following arguments:\n\n\n\nn_estimators: the number of trees in the forest. I set this to 100.\n\n\nmax_depth: the maximum depth of the tree. I set this to 5.\n\n\nrandom_state: the seed used by the random number generator. I set this to 1.\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nn_estimators = 100\nmax_depth = None  # or a specific value\nmin_samples_split = 2\nmin_samples_leaf = 1\nmax_features = None  # or another value like 'sqrt' or 'log2'\n\n# Create the RandomForestClassifier instance\nrf_model = RandomForestClassifier(\n    n_estimators=n_estimators,\n    max_depth=max_depth,\n    min_samples_split=min_samples_split,\n    min_samples_leaf=min_samples_leaf,\n    max_features=max_features,  # Change 'auto' to None\n    random_state=42  # You can set a random state for reproducibility\n)\n\n# Fit the model to the training data\nrf_model.fit(X_train, y_train)\n\nRandomForestClassifier(max_features=None, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(max_features=None, random_state=42)\n\n\n\nEvaluating the Model\n\n\nNow that the model has been trained, I can proceed to evaluate the model. I used the accuracy_score() classification_report(), confusion_matrix()methods to evaluate the model.\n\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Make predictions on the test set\ny_pred = rf_model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\nconfusion_mat = confusion_matrix(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", confusion_mat)\n\nAccuracy: 0.8473\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.87      0.97      0.92      8504\n           1       0.47      0.16      0.24      1496\n\n    accuracy                           0.85     10000\n   macro avg       0.67      0.57      0.58     10000\nweighted avg       0.81      0.85      0.81     10000\n\nConfusion Matrix:\n [[8227  277]\n [1250  246]]\n\n\n\nAccuracy Score\n\n\nBased on the test results:\n\n\n\nThe accuracy is 84.96%, indicating the overall proportion of correct predictions.\n\n\nThe precision, recall, and F1-score for class 0 (booking not completed) are relatively high, suggesting good performance in predicting this class.\n\n\nThe model struggles with class 1 (booking completed), as indicated by lower precision, recall, and F1-score.\n\n\n\nThe class 1 recall is particularly low (0.18), indicating that the model is not capturing a significant portion of actual positive instances. This is a significant issue because the goal is to predict booking completions. The model is not performing well in this regard.\n\n\nThe reason for this is the class imbalance, recall that from the univariate analysis the majority of booking_complete is 0 indicating that the majority of customers do not complete the booking. The model is biased towards predicting 0 because of the class imbalance. This is a very important insight because it informs the next steps in the project.\n\n\nImproving the Model\n\n\nNow that I have a baseline model, I can proceed to improve the model. I used the following techniques to improve the model:\n\n\nResolving the class imbalance.\nHyperparameter tuning.\n\n\nResolving the Class Imbalance\n\n\nI implemented oversampling the class 1(booking complete) using the Synthetic Minority Over-sampling Technique (SMOTE), which is a popular technique for oversampling imbalanced datasets. I used the SMOTE() method from the imblearn.over_sampling module to oversample the minority class.\n\n\n# Apply SMOTE to the training set\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n# Create and train the RandomForestClassifier on the resampled data\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_resampled, y_resampled)\n\n# Make predictions on the test set\ny_pred = rf_model.predict(X_test)\n\n\nRe-Evaluating the Model\n\n\nNow that the model has been trained with oversampled data, I can proceed to re-evaluate the model. I used the accuracy_score() classification_report() and confusion_matrix() methods to evaluate the model.\n\n\n# Evaluate the model\ny_pred = rf_model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\nconfusion_mat = confusion_matrix(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", confusion_mat)\n\nAccuracy: 0.8373\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.87      0.95      0.91      8504\n           1       0.42      0.22      0.28      1496\n\n    accuracy                           0.84     10000\n   macro avg       0.64      0.58      0.60     10000\nweighted avg       0.80      0.84      0.81     10000\n\nConfusion Matrix:\n [[8050  454]\n [1173  323]]\n\n\n\nIn the second test (with oversampling):\n\n\n\n\nThe precision for class 1 has improved from 0.47 to 0.42, indicating an improvement in the proportion of correct positive predictions., it means that the model is making fewer false positive predictions for the “completed” class after applying oversampling. This is generally a positive outcome, as it suggests an improvement in the accuracy of positive predictions\n\n\n\n\nThe recall for class 1 has increased from 0.16 to 0.22, indicating an improvement in capturing actual positive instances of booking completions\n\n\n\nImproving Overall Accuracy With Hyperparameter Tuning\n\n\nNow that the model has been trained with oversampled data, I can proceed to improve the model. I used the following techniques to improve the model with hyperparemeter Tuning\n\n\nHyperparameter tuning is the process of finding the best combination of hyperparameters for a machine learning algorithm that results in the best performance. Hyperparameters are variables that are set before training a model. They control the learning process and the resulting model.\n\n\nThe Random Forest algorithm has several hyperparameters that can be tuned to improve the model’s performance. I used the GridSearchCV() method from the sklearn.model_selection module to tune the hyperparameters. The GridSearchCV() method takes the following arguments:\n\n\n\nShow the code\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import HalvingGridSearchCV\n\n# Define the parameter grid to search\nparam_grid = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2']\n}\n\n# Create a RandomForestClassifier\nrf_model = RandomForestClassifier(random_state=42)\n\n# Create HalvingGridSearchCV object\nhalving_search = HalvingGridSearchCV(\n    estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', factor=3\n)\n\n# Fit the model to the data\nhalving_search.fit(X_resampled, y_resampled)\n\n# Get the best parameters\nbest_params = halving_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n\n\nGrideSearchCV() is a method that performs an exhaustive search over specified parameter values for an estimator."
  }
]